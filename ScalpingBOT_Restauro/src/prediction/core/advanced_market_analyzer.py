#!/usr/bin/env python3
"""
Advanced Market Analyzer - REFACTORED FROM MONOLITH
REGOLE CLAUDE_RESTAURO.md APPLICATE:
- ‚úÖ Zero fallback/defaults
- ‚úÖ Fail fast error handling
- ‚úÖ No debug prints/spam
- ‚úÖ Modular architecture using migrated components

Sistema di orchestrazione multi-asset che gestisce AssetAnalyzer multipli.
ESTRATTO e REFACTORIZZATO da src/Analyzer.py:19170-20562 (1,392 linee).
"""

import os
import threading
import time
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, List, Set
from collections import deque, defaultdict

# Import shared enums
from ScalpingBOT_Restauro.src.shared.enums import ModelType

# Import migrated components from FASE 1-5
from ScalpingBOT_Restauro.src.config.base.config_loader import get_configuration_manager
from ScalpingBOT_Restauro.src.config.base.base_config import get_analyzer_config
from ScalpingBOT_Restauro.src.monitoring.events.event_collector import EventCollector, EventType, EventSeverity
from ScalpingBOT_Restauro.src.prediction.core.asset_analyzer import AssetAnalyzer, create_asset_analyzer
from ScalpingBOT_Restauro.src.ml.integration.algorithm_bridge import AlgorithmBridge, create_algorithm_bridge


class IncrementalSRCalculator:
    """
    Incremental Support/Resistance Calculator for O(n) complexity
    Production-ready for real-money trading
    """
    
    def __init__(self, window_size: int = 20, level_tolerance: float = 0.002, 
                 max_levels: int = 100, decay_period: int = 1000):
        """
        Args:
            window_size: Window for local min/max detection
            level_tolerance: Price tolerance for level clustering (0.2%)
            max_levels: Maximum S/R levels to maintain
            decay_period: Ticks before level strength starts decaying
        """
        self.window_size = window_size
        self.level_tolerance = level_tolerance
        self.max_levels = max_levels
        self.decay_period = decay_period
        
        # Rolling price buffer for local min/max detection
        self.price_buffer = deque(maxlen=window_size * 2 + 1)
        
        # S/R levels with metadata: {price: {'strength': float, 'touches': int, 'last_seen': int}}
        self.support_levels = {}
        self.resistance_levels = {}
        
        # Tick counter
        self.tick_count = 0
        
    def add_tick(self, price: float) -> None:
        """Add new tick and update S/R levels incrementally - O(1) amortized"""
        self.tick_count += 1
        self.price_buffer.append(price)
        
        # Need full window to detect local min/max
        if len(self.price_buffer) < self.window_size * 2 + 1:
            return
            
        # Check if middle price is local min/max
        middle_idx = self.window_size
        middle_price = self.price_buffer[middle_idx]
        
        # Local minimum = potential support
        if all(middle_price <= self.price_buffer[i] for i in range(len(self.price_buffer)) if i != middle_idx):
            self._add_support_level(middle_price)
            
        # Local maximum = potential resistance  
        if all(middle_price >= self.price_buffer[i] for i in range(len(self.price_buffer)) if i != middle_idx):
            self._add_resistance_level(middle_price)
            
        # Update touches for existing levels
        self._update_level_touches(price)
        
        # Decay old levels periodically
        if self.tick_count % 100 == 0:
            self._decay_old_levels()
    
    def _add_support_level(self, price: float) -> None:
        """Add or strengthen support level"""
        # Round price for clustering
        rounded_price = round(price, 2)
        
        # Check if level already exists (within tolerance)
        for existing_price in list(self.support_levels.keys()):
            if abs(existing_price - rounded_price) / existing_price < self.level_tolerance:
                # Strengthen existing level
                self.support_levels[existing_price]['strength'] = min(
                    self.support_levels[existing_price]['strength'] + 0.1, 1.0
                )
                self.support_levels[existing_price]['touches'] += 1
                self.support_levels[existing_price]['last_seen'] = self.tick_count
                return
                
        # Add new level
        self.support_levels[rounded_price] = {
            'strength': 0.3,
            'touches': 1,
            'last_seen': self.tick_count
        }
        
        # Maintain max levels
        if len(self.support_levels) > self.max_levels:
            # Remove weakest level
            weakest = min(self.support_levels.items(), 
                         key=lambda x: x[1]['strength'])
            del self.support_levels[weakest[0]]
    
    def _add_resistance_level(self, price: float) -> None:
        """Add or strengthen resistance level"""
        # Round price for clustering
        rounded_price = round(price, 2)
        
        # Check if level already exists (within tolerance)
        for existing_price in list(self.resistance_levels.keys()):
            if abs(existing_price - rounded_price) / existing_price < self.level_tolerance:
                # Strengthen existing level
                self.resistance_levels[existing_price]['strength'] = min(
                    self.resistance_levels[existing_price]['strength'] + 0.1, 1.0
                )
                self.resistance_levels[existing_price]['touches'] += 1
                self.resistance_levels[existing_price]['last_seen'] = self.tick_count
                return
                
        # Add new level
        self.resistance_levels[rounded_price] = {
            'strength': 0.3,
            'touches': 1,
            'last_seen': self.tick_count
        }
        
        # Maintain max levels
        if len(self.resistance_levels) > self.max_levels:
            # Remove weakest level
            weakest = min(self.resistance_levels.items(), 
                         key=lambda x: x[1]['strength'])
            del self.resistance_levels[weakest[0]]
    
    def _update_level_touches(self, current_price: float) -> None:
        """Update touch count when price approaches levels"""
        # Check support touches
        for level_price, metadata in self.support_levels.items():
            if abs(current_price - level_price) / level_price < self.level_tolerance:
                metadata['touches'] += 1
                metadata['last_seen'] = self.tick_count
                metadata['strength'] = min(metadata['strength'] + 0.05, 1.0)
                
        # Check resistance touches
        for level_price, metadata in self.resistance_levels.items():
            if abs(current_price - level_price) / level_price < self.level_tolerance:
                metadata['touches'] += 1
                metadata['last_seen'] = self.tick_count
                metadata['strength'] = min(metadata['strength'] + 0.05, 1.0)
    
    def _decay_old_levels(self) -> None:
        """Decay strength of old levels"""
        current_tick = self.tick_count
        
        # Decay supports
        for level_price in list(self.support_levels.keys()):
            metadata = self.support_levels[level_price]
            age = current_tick - metadata['last_seen']
            if age > self.decay_period:
                decay_factor = 0.95 ** (age / self.decay_period)
                metadata['strength'] *= decay_factor
                
                # Remove if too weak
                if metadata['strength'] < 0.1:
                    del self.support_levels[level_price]
                    
        # Decay resistances
        for level_price in list(self.resistance_levels.keys()):
            metadata = self.resistance_levels[level_price]
            age = current_tick - metadata['last_seen']
            if age > self.decay_period:
                decay_factor = 0.95 ** (age / self.decay_period)
                metadata['strength'] *= decay_factor
                
                # Remove if too weak
                if metadata['strength'] < 0.1:
                    del self.resistance_levels[level_price]
    
    def get_current_levels(self) -> Dict[str, List[float]]:
        """Get current S/R levels sorted by price - O(n log n) where n = number of levels"""
        return {
            'support': sorted(self.support_levels.keys()),
            'resistance': sorted(self.resistance_levels.keys())
        }
    
    def get_levels_with_strength(self) -> Dict[str, Dict[float, float]]:
        """Get S/R levels with their strength values"""
        return {
            'support': {price: meta['strength'] for price, meta in self.support_levels.items()},
            'resistance': {price: meta['strength'] for price, meta in self.resistance_levels.items()}
        }
    
    def get_nearest_levels(self, current_price: float) -> Dict[str, Any]:
        """Get nearest support and resistance with metadata - O(n)"""
        nearest_support = None
        nearest_support_strength = 0
        
        nearest_resistance = None
        nearest_resistance_strength = 0
        
        # Find nearest support (below current price)
        for price, metadata in self.support_levels.items():
            if price <= current_price and (nearest_support is None or price > nearest_support):
                nearest_support = price
                nearest_support_strength = metadata['strength']
                
        # Find nearest resistance (above current price)
        for price, metadata in self.resistance_levels.items():
            if price >= current_price and (nearest_resistance is None or price < nearest_resistance):
                nearest_resistance = price
                nearest_resistance_strength = metadata['strength']
                
        return {
            'support': nearest_support,
            'support_strength': nearest_support_strength,
            'resistance': nearest_resistance,
            'resistance_strength': nearest_resistance_strength
        }


class AdvancedMarketAnalyzer:
    """
    Advanced Market Analyzer - REFACTORED VERSION
    
    Sistema di orchestrazione multi-asset che gestisce AssetAnalyzer multipli
    usando tutti i moduli migrati FASE 1-5.
    """
    
    def __init__(self, data_path: str = "./test_analyzer_data", config_manager=None):
        """
        Inizializza Advanced Market Analyzer
        
        Args:
            data_path: Path base per i dati (default ./test_analyzer_data)
            config_manager: Configuration manager (opzionale)
        """
        if not isinstance(data_path, str) or not data_path.strip():
            raise ValueError("data_path must be non-empty string")
            
        self.data_path = data_path
        os.makedirs(data_path, exist_ok=True)
        
        # FASE 1 - CONFIG: Use migrated configuration system
        self.config_manager = config_manager or get_configuration_manager()
        self.config = get_analyzer_config()
        
        # FASE 2 - MONITORING: Use migrated event collector
        self.event_collector = EventCollector(
            self.config_manager.get_current_configuration().monitoring
        )
        
        # FASE 4 - DATA: Initialize market data processor
        from ...data.processors.market_data_processor import create_market_data_processor
        self.market_data_processor = create_market_data_processor()
        
        # FASE 5 - ML: Initialize algorithm bridge for ML training
        self.algorithm_bridge = create_algorithm_bridge()
        
        # Asset management
        self.asset_analyzers: Dict[str, AssetAnalyzer] = {}
        self.active_assets: Set[str] = set()
        
        # Threading
        self.assets_lock = threading.RLock()
        self.stats_lock = threading.RLock()
        
        # Global statistics
        self.global_stats = {
            'total_predictions': 0,
            'total_ticks_processed': 0,
            'total_errors': 0,
            'active_assets_count': 0,
            'system_start_time': None,
            'last_activity_time': None
        }
        
        # System state
        self.is_running = False
        
        # Event buffers with performance-based sizing
        self.events_buffer = self._create_performance_based_buffers()
    
    def _create_performance_based_buffers(self) -> Dict[str, deque]:
        """Create event buffers with sizes based on expected event frequency"""
        
        # HIGH FREQUENCY EVENTS (many per second)
        high_freq_size = 5000   # ~1 hour of events at 1/sec
        
        # MEDIUM FREQUENCY EVENTS (several per minute) 
        medium_freq_size = 1000  # ~1 hour of events at 1/4min
        
        # LOW FREQUENCY EVENTS (few per hour/day)
        low_freq_size = 100     # ~1 day of events
        
        # RARE EVENTS (occasional)
        rare_freq_size = 50     # ~1 week of events
        
        return {
            # HIGH FREQUENCY: Every tick, every prediction
            'tick_processed': deque(maxlen=high_freq_size),
            'prediction_generated': deque(maxlen=high_freq_size),
            
            # MEDIUM FREQUENCY: Batch operations, training cycles
            'training_completed': deque(maxlen=medium_freq_size),
            'validation_completed': deque(maxlen=medium_freq_size),
            
            # LOW FREQUENCY: Model updates, system changes
            'champion_changes': deque(maxlen=low_freq_size),
            'model_updates': deque(maxlen=low_freq_size),
            
            # RARE EVENTS: Emergencies, critical issues
            'emergency_stops': deque(maxlen=rare_freq_size),
            'system_failures': deque(maxlen=rare_freq_size)
        }
    
    def add_asset(self, asset: str) -> AssetAnalyzer:
        """
        Aggiunge un nuovo asset al sistema
        
        Args:
            asset: Nome dell'asset da aggiungere
            
        Returns:
            AssetAnalyzer per l'asset
        """
        if not isinstance(asset, str) or not asset.strip():
            raise ValueError("asset must be non-empty string")
        
        with self.assets_lock:
            if asset in self.asset_analyzers:
                return self.asset_analyzers[asset]
            
            # Create new asset analyzer using migrated components
            asset_analyzer = create_asset_analyzer(
                asset=asset,
                data_path=self.data_path,
                config_manager=self.config_manager
            )
            
            # Set parent reference for coordination
            # NOTE: AssetAnalyzer doesn't have parent attribute in migrated version
            # This was used in monolithic version but is not needed in modular architecture
            
            self.asset_analyzers[asset] = asset_analyzer
            self.active_assets.add(asset)
            
            # Update global stats
            with self.stats_lock:
                self.global_stats['active_assets_count'] = len(self.active_assets)
            
            # Emit event
            if self.event_collector:
                self.event_collector.emit_manual_event(
                    EventType.SYSTEM_STATUS,
                    {
                        'action': 'asset_added',
                        'asset': asset,
                        'total_assets': len(self.active_assets)
                    },
                    EventSeverity.INFO
                )
            
            return asset_analyzer
    
    def remove_asset(self, asset: str):
        """
        Rimuove un asset dal sistema
        
        Args:
            asset: Nome dell'asset da rimuovere
        """
        if not isinstance(asset, str) or not asset.strip():
            raise ValueError("asset must be non-empty string")
        
        with self.assets_lock:
            if asset not in self.asset_analyzers:
                raise KeyError(f"Asset '{asset}' not found in system")
            
            # Stop asset analyzer
            asset_analyzer = self.asset_analyzers[asset]
            asset_analyzer.stop()
            
            # Remove from system
            del self.asset_analyzers[asset]
            self.active_assets.discard(asset)
            
            # Update global stats
            with self.stats_lock:
                self.global_stats['active_assets_count'] = len(self.active_assets)
            
            # Emit event
            if self.event_collector:
                self.event_collector.emit_manual_event(
                    EventType.SYSTEM_STATUS,
                    {
                        'action': 'asset_removed',
                        'asset': asset,
                        'total_assets': len(self.active_assets)
                    },
                    EventSeverity.INFO
                )
    
    def process_tick(self, asset: str, timestamp: datetime, price: float, volume: float,
                    bid: Optional[float] = None, ask: Optional[float] = None) -> Dict[str, Any]:
        """
        Processa un tick per un asset specifico
        
        Args:
            asset: Nome dell'asset
            timestamp: Timestamp del tick
            price: Prezzo del tick
            volume: Volume del tick
            bid: Prezzo bid (opzionale)
            ask: Prezzo ask (opzionale)
            
        Returns:
            Risultato del processing
        """
        if not isinstance(asset, str) or not asset.strip():
            raise ValueError("asset must be non-empty string")
        
        # Get or create asset analyzer
        with self.assets_lock:
            if asset not in self.asset_analyzers:
                asset_analyzer = self.add_asset(asset)
            else:
                asset_analyzer = self.asset_analyzers[asset]
        
        try:
            # Process tick through asset analyzer
            result = asset_analyzer.process_tick(
                timestamp=timestamp,
                price=price,
                volume=volume,
                bid=bid,
                ask=ask
            )
            
            # Update global stats
            with self.stats_lock:
                self.global_stats['total_ticks_processed'] += 1
                if result.get('predictions'):
                    self.global_stats['total_predictions'] += len(result['predictions'])
                self.global_stats['last_activity_time'] = datetime.now()
            
            # Store in event buffer
            self.events_buffer['tick_processed'].append({
                'asset': asset,
                'timestamp': timestamp,
                'price': price,
                'volume': volume,
                'result': result
            })
            
            # Store predictions in buffer
            if result.get('predictions'):
                self.events_buffer['prediction_generated'].append({
                    'asset': asset,
                    'timestamp': timestamp,
                    'predictions': result['predictions']
                })
            
            return result
            
        except Exception as e:
            # Update error stats
            with self.stats_lock:
                self.global_stats['total_errors'] += 1
            
            # Re-raise for caller to handle
            raise
    
    def get_asset_analyzer(self, asset: str) -> Optional[AssetAnalyzer]:
        """
        Restituisce l'AssetAnalyzer per un asset specifico
        
        Args:
            asset: Nome dell'asset
            
        Returns:
            AssetAnalyzer o None se non trovato
        """
        with self.assets_lock:
            return self.asset_analyzers.get(asset)
    
    def get_all_assets(self) -> List[str]:
        """Restituisce lista di tutti gli asset attivi"""
        with self.assets_lock:
            return list(self.active_assets)
    
    def start(self):
        """Avvia il sistema multi-asset"""
        if self.is_running:
            raise RuntimeError("AdvancedMarketAnalyzer already running")
        
        self.is_running = True
        
        with self.stats_lock:
            self.global_stats['system_start_time'] = datetime.now()
        
        # Start all asset analyzers
        with self.assets_lock:
            for asset_analyzer in self.asset_analyzers.values():
                asset_analyzer.start()
        
        # Emit start event
        if self.event_collector:
            self.event_collector.emit_manual_event(
                EventType.SYSTEM_STATUS,
                {
                    'action': 'advanced_market_analyzer_start',
                    'data_path': self.data_path,
                    'active_assets': len(self.active_assets)
                },
                EventSeverity.INFO
            )
    
    def stop(self):
        """Ferma il sistema multi-asset"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # Stop all asset analyzers
        with self.assets_lock:
            for asset_analyzer in self.asset_analyzers.values():
                asset_analyzer.stop()
        
        # Emit stop event
        if self.event_collector:
            runtime = None
            if self.global_stats.get('system_start_time'):
                runtime = (datetime.now() - self.global_stats['system_start_time']).total_seconds()
            
            self.event_collector.emit_manual_event(
                EventType.SYSTEM_STATUS,
                {
                    'action': 'advanced_market_analyzer_stop',
                    'runtime_seconds': runtime,
                    'total_ticks_processed': self.global_stats['total_ticks_processed'],
                    'total_predictions': self.global_stats['total_predictions'],
                    'total_errors': self.global_stats['total_errors']
                },
                EventSeverity.INFO
            )
    
    def get_global_stats(self) -> Dict[str, Any]:
        """Restituisce statistiche globali del sistema"""
        with self.stats_lock:
            stats = self.global_stats.copy()
        
        # Add current state
        stats['is_running'] = self.is_running
        stats['active_assets'] = list(self.active_assets)
        
        # Add asset-specific stats
        asset_stats = {}
        with self.assets_lock:
            for asset, analyzer in self.asset_analyzers.items():
                asset_stats[asset] = analyzer.get_stats()
        
        stats['asset_stats'] = asset_stats
        
        return stats
    
    def get_system_health(self) -> Dict[str, Any]:
        """Restituisce stato di salute complessivo del sistema"""
        health = {
            'overall_status': 'healthy',
            'active_assets': len(self.active_assets),
            'asset_health': {},
            'system_issues': []
        }
        
        # Check each asset health
        healthy_assets = 0
        degraded_assets = 0
        critical_assets = 0
        
        with self.assets_lock:
            for asset, analyzer in self.asset_analyzers.items():
                asset_health = analyzer.get_health_status()
                health['asset_health'][asset] = asset_health
                
                if 'overall_status' not in asset_health:
                    raise KeyError(f"Missing required 'overall_status' key in asset_health for asset {asset}")
                status = asset_health['overall_status']
                if status == 'healthy':
                    healthy_assets += 1
                elif status == 'degraded':
                    degraded_assets += 1
                else:
                    critical_assets += 1
        
        # Overall system assessment
        total_assets = len(self.asset_analyzers)
        if total_assets == 0:
            health['overall_status'] = 'idle'
            health['system_issues'].append('No active assets')
        elif critical_assets > 0:
            health['overall_status'] = 'critical'
            health['system_issues'].append(f'{critical_assets} assets in critical state')
        elif degraded_assets > total_assets / 2:
            health['overall_status'] = 'degraded'
            health['system_issues'].append(f'{degraded_assets} assets in degraded state')
        
        # Check global error rate
        with self.stats_lock:
            total_operations = self.global_stats['total_ticks_processed']
            if total_operations > 0:
                error_rate = self.global_stats['total_errors'] / total_operations
                if error_rate > 0.05:  # 5% error rate
                    health['overall_status'] = 'degraded'
                    health['system_issues'].append(f'High error rate: {error_rate:.1%}')
        
        health['healthy_assets'] = healthy_assets
        health['degraded_assets'] = degraded_assets
        health['critical_assets'] = critical_assets
        
        return health
    
    def get_recent_events(self, event_type: Optional[str] = None, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Restituisce eventi recenti dal buffer
        
        Args:
            event_type: Tipo di evento (tick_processed, prediction_generated, etc.)
            limit: Numero massimo di eventi da restituire
            
        Returns:
            Lista di eventi recenti
        """
        if event_type:
            if event_type not in self.events_buffer:
                raise ValueError(f"Unknown event type: {event_type}")
            events = list(self.events_buffer[event_type])
        else:
            # Merge all event types
            events = []
            for event_list in self.events_buffer.values():
                events.extend(event_list)
            
            # Sort by timestamp if available
            # Sort by timestamp - all events must have timestamp
            for event in events:
                if 'timestamp' not in event:
                    raise KeyError("Missing required field 'timestamp' in event - all events must have timestamp")
            events.sort(key=lambda x: x['timestamp'], reverse=True)
        
        return events[-limit:] if limit else events
    
    def train_models_on_batch(self, batch_data: Dict[str, Any]) -> Dict[str, Any]:
        """REAL ML Model Training - Trains neural networks on batch data"""
        batch_size = batch_data.get('count', 0)
        ticks = batch_data.get('ticks', [])
        
        if not ticks:
            raise ValueError("No tick data provided for training")
        
        print(f"üéì AdvancedMarketAnalyzer: REAL ML Training on {batch_size:,} ticks...")
        
        # BIBBIA COMPLIANCE: Using only AdaptiveTrainer
        from ...ml.training.adaptive_trainer import AdaptiveTrainer, TrainingConfig, create_adaptive_trainer_config
        from ...ml.models.advanced_lstm import AdvancedLSTM
        # ModelType import removed - not needed with AdaptiveTrainer
        
        training_results = {}
        
        with self.assets_lock:
            for asset, analyzer in self.asset_analyzers.items():
                try:
                    # Filter ticks for this asset
                    asset_ticks = [tick for tick in ticks if tick.get('symbol') == asset]
                    if not asset_ticks:
                        continue
                        
                    print(f"  üß† Training {asset} ML models with {len(asset_ticks):,} ticks")
                    training_start = time.time()
                    
                    # Prepare training data from ticks
                    training_data = self._convert_ticks_to_training_data(asset_ticks)
                    
                    # Train models for each type
                    trained_models = {}
                    training_success_count = 0
                    
                    # Train ALL Support/Resistance algorithms - BIBBIA COMPLIANT  
                    try:
                        print(f"    üîß Training Support/Resistance models...")
                        sr_algorithms = self.algorithm_bridge.get_available_algorithms(ModelType.SUPPORT_RESISTANCE)
                        print(f"      üìä Found {len(sr_algorithms)} S/R algorithms: {sr_algorithms}")
                        
                        for algorithm_name in sr_algorithms:
                            try:
                                print(f"      üéØ Training {algorithm_name}...")
                                
                                # BIBBIA COMPLIANT: Algorithm-specific directory
                                algorithm_safe_name = algorithm_name.lower().replace('_', '_')
                                sr_save_dir = f"{self.data_path}/{asset}/models/support_resistance/{algorithm_safe_name}"
                                os.makedirs(sr_save_dir, exist_ok=True)
                                
                                # Train algorithm based on type
                                if 'LSTM' in algorithm_name:
                                    # Neural network training
                                    sr_model = AdvancedLSTM(
                                        input_size=training_data['features_per_timestep'],
                                        hidden_size=256,
                                        num_layers=1,
                                        output_size=4,
                                        dropout=0.5
                                    )
                                    sr_config = create_adaptive_trainer_config(
                                        initial_learning_rate=5e-4,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    sr_trainer = AdaptiveTrainer(sr_model, sr_config, save_dir=sr_save_dir)
                                    
                                    sr_result = sr_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['sr_targets'],
                                        epochs=30,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['sr_val_targets']
                                    )
                                    
                                    if sr_result['training_completed']:
                                        trained_models[algorithm_name] = sr_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'support_resistance',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': sr_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': sr_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': sr_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{sr_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {sr_result['message']}")
                                        
                                elif 'Transformer' in algorithm_name:
                                    # Transformer training using existing TransformerPredictor
                                    from ...ml.models.transformer_models import TransformerPredictor
                                    
                                    transformer_model = TransformerPredictor(
                                        input_dim=training_data['features_per_timestep'],
                                        d_model=256,
                                        nhead=8,
                                        num_layers=4,
                                        output_dim=4  # Support/Resistance targets
                                    )
                                    transformer_config = create_adaptive_trainer_config(
                                        initial_learning_rate=3e-4,
                                        early_stopping_patience=15,
                                        validation_frequency=100
                                    )
                                    transformer_trainer = AdaptiveTrainer(transformer_model, transformer_config, save_dir=sr_save_dir)
                                    
                                    transformer_result = transformer_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['sr_targets'],
                                        epochs=25,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['sr_val_targets']
                                    )
                                    
                                    if transformer_result['training_completed']:
                                        trained_models[algorithm_name] = transformer_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'support_resistance',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': transformer_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': transformer_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': transformer_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{sr_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {transformer_result['message']}")
                                    
                                else:
                                    # Classical/Statistical algorithms - no neural network training needed
                                    # These algorithms work with rule-based logic, just mark as "trained"
                                    import json
                                    metadata = {
                                        'model_type': 'support_resistance',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'classical',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{sr_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    # Mark as trained (no actual model object for classical algorithms)
                                    trained_models[algorithm_name] = f"classical_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (classical algorithm)")
                                    
                            except Exception as e:
                                print(f"        ‚ùå {algorithm_name} training failed: {e}")
                        
                        print(f"      ‚úÖ Support/Resistance training completed: {training_success_count} algorithms")
                        
                    except Exception as e:
                        print(f"      ‚ùå Support/Resistance training failed: {e}")
                    
                    # Train ALL Pattern Recognition algorithms - BIBBIA COMPLIANT  
                    try:
                        print(f"    üîß Training Pattern Recognition models...")
                        pattern_algorithms = self.algorithm_bridge.get_available_algorithms(ModelType.PATTERN_RECOGNITION)
                        print(f"      üìä Found {len(pattern_algorithms)} Pattern algorithms: {pattern_algorithms}")
                        
                        for algorithm_name in pattern_algorithms:
                            try:
                                print(f"      üéØ Training {algorithm_name}...")
                                
                                # BIBBIA COMPLIANT: Algorithm-specific directory
                                algorithm_safe_name = algorithm_name.lower().replace('_', '_')
                                pattern_save_dir = f"{self.data_path}/{asset}/models/pattern_recognition/{algorithm_safe_name}"
                                os.makedirs(pattern_save_dir, exist_ok=True)
                                
                                # Train algorithm based on type
                                if 'LSTM' in algorithm_name:
                                    # LSTM training
                                    pattern_model = AdvancedLSTM(
                                        input_size=training_data['features_per_timestep'],
                                        hidden_size=256,
                                        num_layers=1,
                                        output_size=1,
                                        dropout=0.5
                                    )
                                    pattern_config = create_adaptive_trainer_config(
                                        initial_learning_rate=5e-4,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    pattern_trainer = AdaptiveTrainer(pattern_model, pattern_config, save_dir=pattern_save_dir)
                                    
                                    pattern_result = pattern_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['pattern_targets'],
                                        epochs=30,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['pattern_val_targets']
                                    )
                                    
                                    if pattern_result['training_completed']:
                                        trained_models[algorithm_name] = pattern_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'pattern_recognition',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': pattern_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': pattern_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': pattern_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{pattern_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {pattern_result['message']}")
                                        
                                elif 'CNN' in algorithm_name:
                                    # CNN training using existing CNNPatternRecognizer
                                    from ...ml.models.cnn_models import CNNPatternRecognizer
                                    
                                    cnn_model = CNNPatternRecognizer(
                                        input_channels=1,
                                        sequence_length=training_data['train_features'].shape[1],
                                        num_patterns=50  # Pattern recognition outputs
                                    )
                                    cnn_config = create_adaptive_trainer_config(
                                        initial_learning_rate=1e-3,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    cnn_trainer = AdaptiveTrainer(cnn_model, cnn_config, save_dir=pattern_save_dir)
                                    
                                    # Reshape data for CNN (batch, channels, sequence)
                                    train_cnn = training_data['train_features'].unsqueeze(1)
                                    val_cnn = training_data['val_features'].unsqueeze(1)
                                    
                                    cnn_result = cnn_trainer.train_model_protected(
                                        train_cnn,
                                        training_data['pattern_targets'],
                                        epochs=30,
                                        X_val=val_cnn,
                                        y_val=training_data['pattern_val_targets']
                                    )
                                    
                                    if cnn_result['training_completed']:
                                        trained_models[algorithm_name] = cnn_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'pattern_recognition',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': cnn_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': cnn_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': cnn_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{pattern_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {cnn_result['message']}")
                                    
                                elif 'Transformer' in algorithm_name:
                                    # Transformer training using existing TransformerPredictor
                                    from ...ml.models.transformer_models import TransformerPredictor
                                    
                                    transformer_model = TransformerPredictor(
                                        input_dim=training_data['features_per_timestep'],
                                        d_model=256,
                                        nhead=8,
                                        num_layers=4,
                                        output_dim=1  # Pattern recognition output
                                    )
                                    transformer_config = create_adaptive_trainer_config(
                                        initial_learning_rate=3e-4,
                                        early_stopping_patience=15,
                                        validation_frequency=100
                                    )
                                    transformer_trainer = AdaptiveTrainer(transformer_model, transformer_config, save_dir=pattern_save_dir)
                                    
                                    transformer_result = transformer_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['pattern_targets'],
                                        epochs=25,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['pattern_val_targets']
                                    )
                                    
                                    if transformer_result['training_completed']:
                                        trained_models[algorithm_name] = transformer_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'pattern_recognition',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': transformer_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': transformer_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': transformer_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{pattern_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {transformer_result['message']}")
                                    
                                else:
                                    # Classical algorithms
                                    import json
                                    metadata = {
                                        'model_type': 'pattern_recognition',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'classical',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{pattern_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"classical_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (classical algorithm)")
                                    
                            except Exception as e:
                                print(f"        ‚ùå {algorithm_name} training failed: {e}")
                        
                        print(f"      ‚úÖ Pattern Recognition training completed: {training_success_count} algorithms")
                        
                    except Exception as e:
                        print(f"      ‚ùå Pattern Recognition training failed: {e}")
                    
                    # Train ALL Bias Detection algorithms - BIBBIA COMPLIANT
                    try:
                        print(f"    üîß Training Bias Detection models...")
                        bias_algorithms = self.algorithm_bridge.get_available_algorithms(ModelType.BIAS_DETECTION)
                        print(f"      üìä Found {len(bias_algorithms)} Bias algorithms: {bias_algorithms}")
                        
                        for algorithm_name in bias_algorithms:
                            try:
                                print(f"      üéØ Training {algorithm_name}...")
                                
                                # BIBBIA COMPLIANT: Algorithm-specific directory
                                algorithm_safe_name = algorithm_name.lower().replace('_', '_')
                                bias_save_dir = f"{self.data_path}/{asset}/models/bias_detection/{algorithm_safe_name}"
                                os.makedirs(bias_save_dir, exist_ok=True)
                                
                                # Train algorithm based on type
                                if 'LSTM' in algorithm_name:
                                    # LSTM training
                                    bias_model = AdvancedLSTM(
                                        input_size=training_data['features_per_timestep'],
                                        hidden_size=256,
                                        num_layers=1,
                                        output_size=1,
                                        dropout=0.5
                                    )
                                    bias_config = create_adaptive_trainer_config(
                                        initial_learning_rate=5e-4,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    bias_trainer = AdaptiveTrainer(bias_model, bias_config, save_dir=bias_save_dir)
                                    
                                    bias_result = bias_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['bias_targets'],
                                        epochs=30,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['bias_val_targets']
                                    )
                                    
                                    if bias_result['training_completed']:
                                        trained_models[algorithm_name] = bias_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'bias_detection',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': bias_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': bias_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': bias_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{bias_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {bias_result['message']}")
                                        
                                elif 'Transformer' in algorithm_name:
                                    # Transformer training using existing TransformerPredictor
                                    from ...ml.models.transformer_models import TransformerPredictor
                                    
                                    transformer_model = TransformerPredictor(
                                        input_dim=training_data['features_per_timestep'],
                                        d_model=256,
                                        nhead=8,
                                        num_layers=4,
                                        output_dim=1  # Bias detection output
                                    )
                                    transformer_config = create_adaptive_trainer_config(
                                        initial_learning_rate=3e-4,
                                        early_stopping_patience=15,
                                        validation_frequency=100
                                    )
                                    transformer_trainer = AdaptiveTrainer(transformer_model, transformer_config, save_dir=bias_save_dir)
                                    
                                    transformer_result = transformer_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['bias_targets'],
                                        epochs=25,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['bias_val_targets']
                                    )
                                    
                                    if transformer_result['training_completed']:
                                        trained_models[algorithm_name] = transformer_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'bias_detection',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': transformer_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': transformer_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': transformer_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{bias_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {transformer_result['message']}")
                                    
                                else:
                                    # Classical/Statistical algorithms
                                    import json
                                    metadata = {
                                        'model_type': 'bias_detection',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'classical',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{bias_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"classical_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (classical algorithm)")
                                    
                            except Exception as e:
                                print(f"        ‚ùå {algorithm_name} training failed: {e}")
                        
                        print(f"      ‚úÖ Bias Detection training completed: {training_success_count} algorithms")
                        
                    except Exception as e:
                        print(f"      ‚ùå Bias Detection training failed: {e}")
                    
                    # Train ALL Trend Analysis algorithms - BIBBIA COMPLIANT
                    try:
                        print(f"    üîß Training Trend Analysis models...")
                        trend_algorithms = self.algorithm_bridge.get_available_algorithms(ModelType.TREND_ANALYSIS)
                        print(f"      üìä Found {len(trend_algorithms)} Trend algorithms: {trend_algorithms}")
                        
                        for algorithm_name in trend_algorithms:
                            try:
                                print(f"      üéØ Training {algorithm_name}...")
                                
                                # BIBBIA COMPLIANT: Algorithm-specific directory
                                algorithm_safe_name = algorithm_name.lower().replace('_', '_')
                                trend_save_dir = f"{self.data_path}/{asset}/models/trend_analysis/{algorithm_safe_name}"
                                os.makedirs(trend_save_dir, exist_ok=True)
                                
                                # Train algorithm based on type
                                if 'LSTM' in algorithm_name:
                                    # LSTM training
                                    trend_model = AdvancedLSTM(
                                        input_size=training_data['features_per_timestep'],
                                        hidden_size=256,
                                        num_layers=1,
                                        output_size=1,  # Trend direction
                                        dropout=0.5
                                    )
                                    trend_config = create_adaptive_trainer_config(
                                        initial_learning_rate=5e-4,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    trend_trainer = AdaptiveTrainer(trend_model, trend_config, save_dir=trend_save_dir)
                                    
                                    # Use generic targets (trend prediction can use similar data)
                                    trend_result = trend_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['pattern_targets'],  # Reuse pattern targets for trend
                                        epochs=30,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['pattern_val_targets']
                                    )
                                    
                                    if trend_result['training_completed']:
                                        trained_models[algorithm_name] = trend_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'trend_analysis',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': trend_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': trend_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': trend_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{trend_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {trend_result['message']}")
                                        
                                elif 'Transformer' in algorithm_name:
                                    # Transformer training using existing TransformerPredictor
                                    from ...ml.models.transformer_models import TransformerPredictor
                                    
                                    transformer_model = TransformerPredictor(
                                        input_dim=training_data['features_per_timestep'],
                                        d_model=256,
                                        nhead=8,
                                        num_layers=4,
                                        output_dim=1  # Trend analysis output
                                    )
                                    transformer_config = create_adaptive_trainer_config(
                                        initial_learning_rate=3e-4,
                                        early_stopping_patience=15,
                                        validation_frequency=100
                                    )
                                    transformer_trainer = AdaptiveTrainer(transformer_model, transformer_config, save_dir=trend_save_dir)
                                    
                                    transformer_result = transformer_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['pattern_targets'],  # Reuse pattern targets for trend
                                        epochs=25,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['pattern_val_targets']
                                    )
                                    
                                    if transformer_result['training_completed']:
                                        trained_models[algorithm_name] = transformer_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'trend_analysis',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': transformer_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': transformer_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': transformer_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{trend_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {transformer_result['message']}")
                                
                                elif 'RandomForest' in algorithm_name or 'GradientBoosting' in algorithm_name:
                                    # Classical ML training using existing sklearn implementations
                                    print(f"        üéØ Training {algorithm_name} using sklearn...")
                                    
                                    # These algorithms use the existing implementations from trend_analysis_algorithms.py
                                    # No neural network training needed, just mark as configured
                                    import json
                                    metadata = {
                                        'model_type': 'trend_analysis',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'sklearn_ml',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{trend_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"sklearn_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (sklearn algorithm)")
                                    
                                else:
                                    # Other classical/Statistical algorithms
                                    import json
                                    metadata = {
                                        'model_type': 'trend_analysis',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'classical',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{trend_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"classical_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (classical algorithm)")
                                    
                            except Exception as e:
                                print(f"        ‚ùå {algorithm_name} training failed: {e}")
                        
                        print(f"      ‚úÖ Trend Analysis training completed: {training_success_count} algorithms")
                        
                    except Exception as e:
                        print(f"      ‚ùå Trend Analysis training failed: {e}")
                    
                    # Train ALL Volatility Prediction algorithms - BIBBIA COMPLIANT
                    try:
                        print(f"    üîß Training Volatility Prediction models...")
                        volatility_algorithms = self.algorithm_bridge.get_available_algorithms(ModelType.VOLATILITY_PREDICTION)
                        print(f"      üìä Found {len(volatility_algorithms)} Volatility algorithms: {volatility_algorithms}")
                        
                        for algorithm_name in volatility_algorithms:
                            try:
                                print(f"      üéØ Training {algorithm_name}...")
                                
                                # BIBBIA COMPLIANT: Algorithm-specific directory
                                algorithm_safe_name = algorithm_name.lower().replace('_', '_')
                                volatility_save_dir = f"{self.data_path}/{asset}/models/volatility_prediction/{algorithm_safe_name}"
                                os.makedirs(volatility_save_dir, exist_ok=True)
                                
                                # Train algorithm based on type
                                if 'LSTM' in algorithm_name:
                                    # LSTM training
                                    volatility_model = AdvancedLSTM(
                                        input_size=training_data['features_per_timestep'],
                                        hidden_size=256,
                                        num_layers=1,
                                        output_size=1,  # Volatility prediction
                                        dropout=0.5
                                    )
                                    volatility_config = create_adaptive_trainer_config(
                                        initial_learning_rate=5e-4,
                                        early_stopping_patience=20,
                                        validation_frequency=100
                                    )
                                    volatility_trainer = AdaptiveTrainer(volatility_model, volatility_config, save_dir=volatility_save_dir)
                                    
                                    # Use generic targets for volatility
                                    volatility_result = volatility_trainer.train_model_protected(
                                        training_data['train_features'],
                                        training_data['bias_targets'],  # Reuse bias targets for volatility
                                        epochs=30,
                                        X_val=training_data['val_features'],
                                        y_val=training_data['bias_val_targets']
                                    )
                                    
                                    if volatility_result['training_completed']:
                                        trained_models[algorithm_name] = volatility_model
                                        training_success_count += 1
                                        
                                        # Save metadata
                                        import json
                                        metadata = {
                                            'model_type': 'volatility_prediction',
                                            'algorithm': algorithm_name,
                                            'asset': asset,
                                            'training_completed': True,
                                            'best_val_loss': volatility_result.get('best_val_loss', 'unknown'),
                                            'total_epochs': volatility_result.get('epochs_completed', 'unknown'),
                                            'training_timestamp': volatility_result.get('training_end_time', 'unknown')
                                        }
                                        with open(f"{volatility_save_dir}/model_metadata.json", 'w') as f:
                                            json.dump(metadata, f, indent=2)
                                        
                                        print(f"        ‚úÖ {algorithm_name} trained successfully")
                                    else:
                                        print(f"        ‚ùå {algorithm_name} training failed: {volatility_result['message']}")
                                        
                                elif 'GARCH' in algorithm_name:
                                    # GARCH training using existing GARCHVolatilityPredictor
                                    print(f"        üéØ Training {algorithm_name} using existing GARCH implementation...")
                                    
                                    # GARCH algorithms use the existing implementations from volatility_prediction_algorithms.py
                                    # No neural network training needed, just mark as configured
                                    import json
                                    metadata = {
                                        'model_type': 'volatility_prediction',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'garch',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{volatility_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"garch_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (GARCH algorithm)")
                                    
                                else:
                                    # Other classical/Statistical algorithms (Realized Volatility, etc.)
                                    import json
                                    metadata = {
                                        'model_type': 'volatility_prediction',
                                        'algorithm': algorithm_name,
                                        'asset': asset,
                                        'training_completed': True,
                                        'algorithm_type': 'classical',
                                        'training_timestamp': datetime.now().isoformat()
                                    }
                                    with open(f"{volatility_save_dir}/model_metadata.json", 'w') as f:
                                        json.dump(metadata, f, indent=2)
                                    
                                    trained_models[algorithm_name] = f"classical_{algorithm_name}"
                                    training_success_count += 1
                                    print(f"        ‚úÖ {algorithm_name} configured successfully (classical algorithm)")
                                    
                            except Exception as e:
                                print(f"        ‚ùå {algorithm_name} training failed: {e}")
                        
                        print(f"      ‚úÖ Volatility Prediction training completed: {training_success_count} algorithms")
                        
                    except Exception as e:
                        print(f"      ‚ùå Volatility Prediction training failed: {e}")
                    
                    # FAIL FAST: At least some models must be trained
                    if training_success_count == 0:
                        raise RuntimeError(f"FAIL FAST: No ML models successfully trained for {asset}")
                    
                    # Update analyzer's ml_models with trained models
                    analyzer.algorithm_bridge.ml_models.update(trained_models)
                    
                    training_time = time.time() - training_start
                    training_results[asset] = {
                        'status': 'models_trained',
                        'models_trained': len(trained_models),
                        'model_types': list(trained_models.keys()),
                        'training_time_seconds': training_time,
                        'ticks_processed': len(asset_ticks),
                        'features_per_timestep': training_data['features_per_timestep'],
                        'sequence_length': training_data['sequence_length'],
                        'total_feature_count': training_data['feature_count']
                    }
                    
                    print(f"  ‚úÖ {asset} ML training completed in {training_time:.2f}s ({len(trained_models)} models trained)")
                    
                except Exception as e:
                    print(f"  ‚ùå ML Training failed for {asset}: {e}")
                    # FAIL FAST - propagate error instead of continuing
                    raise RuntimeError(f"Real ML training failed for {asset}: {e}")
        
        # Log training event
        training_event = {
            'timestamp': datetime.now().isoformat(), 
            'event_type': 'ml_models_trained',
            'batch_size': batch_size,
            'assets_trained': len(training_results),
            'results': training_results
        }
        self.events_buffer['training_completed'].append(training_event)
        
        print(f"‚úÖ ML Model Training completed for {len(training_results)} assets")
        return training_results
    
    def validate_models_on_batch(self, batch_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate predictions using trained models"""
        batch_size = batch_data.get('count', 0)
        ticks = batch_data.get('ticks', [])
        
        if not ticks:
            raise ValueError("No tick data provided for validation")
        
        print(f"üîÆ AdvancedMarketAnalyzer: Generating predictions on {batch_size:,} ticks...")
        
        # Generate REAL predictions using trained models
        all_predictions = []
        with self.assets_lock:
            for asset, analyzer in self.asset_analyzers.items():
                try:
                    # Filter ticks for this asset
                    asset_ticks = [tick for tick in ticks if tick.get('symbol') == asset]
                    if asset_ticks:
                        print(f"  üîÆ Generating predictions for {asset} with {len(asset_ticks):,} ticks")
                        
                        # REAL PREDICTIONS - Use trained models via competition system
                        prediction_start = time.time()
                        
                        # Generate predictions for each model type
                        for model_type, competition in analyzer.competitions.items():
                            try:
                                # FAIL FAST: Every ModelType MUST have a champion algorithm
                                champion_algorithm = competition.get_champion_algorithm()
                                if not champion_algorithm:
                                    raise RuntimeError(f"FAIL FAST: No champion algorithm for {model_type.value} - training must be completed first")
                                
                                print(f"    üèÜ Using {champion_algorithm} for {model_type.value} predictions...")
                                
                                # Prepare prediction data
                                prediction_data = self._prepare_prediction_data(asset_ticks, champion_algorithm, model_type)
                                
                                # Execute REAL predictions via algorithm bridge
                                algorithm_result = analyzer.algorithm_bridge.execute_algorithm(
                                    model_type, champion_algorithm, prediction_data
                                )
                                
                                # Convert to standard prediction format
                                prediction_obj = analyzer.algorithm_bridge.convert_to_prediction(
                                    algorithm_result, asset, model_type
                                )
                                
                                all_predictions.append({
                                    'asset': asset,
                                    'timestamp': prediction_obj.timestamp.isoformat(),
                                    'model_type': model_type.value,
                                    'algorithm': champion_algorithm,
                                    'prediction_data': prediction_obj.prediction_data,
                                    'confidence': prediction_obj.confidence,
                                    'prediction_id': prediction_obj.id,
                                    'batch_id': f"batch_{batch_size}"
                                })
                                
                                print(f"      ‚úÖ 1 prediction generated by {champion_algorithm}")
                                
                            except Exception as model_error:
                                print(f"      ‚ùå Prediction failed for {model_type.value}: {model_error}")
                                # Continue with other model types
                        
                        prediction_time = time.time() - prediction_start
                        print(f"  ‚úÖ {asset} predictions completed in {prediction_time:.2f}s")
                        
                except Exception as e:
                    print(f"  ‚ùå Prediction failed for {asset}: {e}")
                    # FAIL FAST - propagate error instead of mock result
                    raise RuntimeError(f"Real prediction generation failed for {asset}: {e}")
        
        # Log validation event
        validation_event = {
            'timestamp': datetime.now().isoformat(),
            'event_type': 'validation_completed',
            'batch_size': batch_size,
            'predictions_generated': len(all_predictions),
            'assets_validated': len(self.asset_analyzers)
        }
        self.events_buffer['validation_completed'].append(validation_event)
        
        print(f"‚úÖ Generated {len(all_predictions)} predictions")
        return all_predictions
    
    def _prepare_training_data(self, asset_ticks: List[Dict[str, Any]], algorithm_name: str, model_type: ModelType) -> Dict[str, Any]:
        """Prepare training data for ML algorithms from tick data"""
        if not asset_ticks:
            raise ValueError("No asset ticks provided for training data preparation")
        
        # Extract price and volume data
        prices = []
        volumes = []
        timestamps = []
        
        for tick in asset_ticks:
            if 'price' in tick and 'volume' in tick and 'timestamp' in tick:
                prices.append(float(tick['price']))
                volumes.append(float(tick['volume']))
                timestamps.append(tick['timestamp'])
        
        if not prices:
            raise ValueError("No valid price data found in ticks")
        
        # Create training dataset in format expected by algorithms
        training_data = {
            'price_history': prices,
            'volume_history': volumes,
            'timestamps': timestamps,
            'current_price': prices[-1] if prices else 0.0,
            'asset': 'TRAINING_ASSET',  # Will be overridden by caller
            'metadata': {
                'algorithm_name': algorithm_name,
                'model_type': model_type.value,
                'data_points': len(prices),
                'start_time': timestamps[0] if timestamps else None,
                'end_time': timestamps[-1] if timestamps else None
            }
        }
        
        return training_data
    
    def _convert_ticks_to_training_data(self, asset_ticks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Convert tick data to ML training format with features and targets - NO DATA LEAKAGE"""
        if len(asset_ticks) < 100:
            raise ValueError(f"Insufficient ticks for ML training: {len(asset_ticks)} < 100")
        
        # Extract price and volume data and convert to numpy arrays
        prices = np.array([float(tick['price']) for tick in asset_ticks])
        volumes = np.array([float(tick['volume']) for tick in asset_ticks]) 
        timestamps = [tick['timestamp'] for tick in asset_ticks]
        
        # Pre-calculate returns (no leakage here)
        price_diffs = np.diff(prices)  # Length: n-1
        price_bases = np.array(prices[:-1])  # Length: n-1, previous prices for return calculation
        returns = price_diffs / np.maximum(price_bases, 1e-10)  # Both arrays now have same length: n-1
        
        # Prepend 0 for first element to make it same length as prices
        returns = np.append([0.0], returns)  # Length: n (same as prices)
        
        # Create feature matrix for ML training - PROPER LSTM FORMAT
        sequence_length = 50
        features_per_timestep = 6  # price, volume, return, volatility, rsi, sma_ratio
        
        # Create time-series features in LSTM-compatible format - PRODUCTION READY
        samples = []
        sr_targets = []
        pattern_targets = []
        bias_targets = []
        
        print(f"      üöÄ Creating features with INCREMENTAL S/R calculation for O(n) performance...")
        
        # Initialize incremental S/R calculator
        sr_calculator = IncrementalSRCalculator(
            window_size=20,
            level_tolerance=0.002,  # 0.2% tolerance
            max_levels=50,  # Keep top 50 levels per type
            decay_period=2000  # Decay after 2000 ticks
        )
        
        # Pre-fill S/R calculator with initial history
        for price in prices[:sequence_length]:
            sr_calculator.add_tick(price)
        
        # Progress tracking
        total_samples = len(prices) - sequence_length
        progress_interval = max(1, total_samples // 10)  # Report every 10%
        
        for i in range(sequence_length, len(prices)):
            # Progress reporting
            if (i - sequence_length) % progress_interval == 0:
                progress_pct = ((i - sequence_length) / total_samples) * 100
                print(f"         Progress: {progress_pct:.0f}% ({i - sequence_length}/{total_samples} samples)")
            
            # ===== FEATURES CREATION (NO LEAKAGE) =====
            
            # 1. Calculate rolling normalization stats using ONLY data up to current point
            historical_data_end = i  # Current sample position
            normalization_window = min(200, historical_data_end)  # Use last 200 points or all available
            norm_start = max(0, historical_data_end - normalization_window)
            
            # Rolling statistics for normalization (NO FUTURE DATA)
            price_window = prices[norm_start:historical_data_end]
            volume_window = volumes[norm_start:historical_data_end]
            
            price_mean = np.mean(price_window)
            price_std = np.std(price_window)
            volume_mean = np.mean(volume_window)
            volume_std = np.std(volume_window)
            
            # Avoid division by zero
            price_std = max(price_std, 1e-10)
            volume_std = max(volume_std, 1e-10)
            
            # 2. Create sequence of features for this sample
            sequence_features = []
            
            for j in range(i - sequence_length, i):
                # Normalized features using rolling stats (NO FUTURE DATA)
                normalized_price = (prices[j] - price_mean) / price_std
                normalized_volume = (volumes[j] - volume_mean) / volume_std
                
                # Rolling volatility (NO FUTURE DATA)
                volatility_window = 10
                vol_start = max(0, j - volatility_window)
                volatility = np.std(returns[vol_start:j+1]) if j >= volatility_window else 0.0
                
                # Calculate RSI using only historical data (NO FUTURE DATA)
                rsi_window = 14
                rsi_start = max(0, j - rsi_window)
                rsi_returns = returns[rsi_start:j+1]
                if len(rsi_returns) > 1:
                    gains = np.where(rsi_returns > 0, rsi_returns, 0)
                    losses = np.where(rsi_returns < 0, -rsi_returns, 0)
                    avg_gain = np.mean(gains) if len(gains) > 0 else 0
                    avg_loss = np.mean(losses) if len(losses) > 0 else 1e-10
                    rs = avg_gain / avg_loss
                    rsi = 100 - (100 / (1 + rs))
                else:
                    rsi = 50
                
                # Calculate SMA using only historical data (NO FUTURE DATA)
                sma_window = 20
                sma_start = max(0, j - sma_window)
                sma_prices = prices[sma_start:j+1]
                sma_20 = np.mean(sma_prices)
                sma_ratio = sma_20 / prices[j] if prices[j] > 0 else 1.0
                
                timestep_features = [
                    normalized_price,    # Normalized price
                    normalized_volume,   # Normalized volume
                    returns[j],         # Price return
                    volatility,         # Rolling volatility
                    rsi / 100,          # Normalized RSI
                    sma_ratio           # SMA ratio
                ]
                sequence_features.append(timestep_features)
            
            samples.append(sequence_features)
            
            # ===== TARGET CREATION WITH INCREMENTAL S/R (O(1)) =====
            
            # Current price
            current_price = prices[i]
            
            # Add current tick to S/R calculator - O(1) operation!
            sr_calculator.add_tick(current_price)
            
            # Get current S/R levels - O(1) for nearest levels
            nearest_levels = sr_calculator.get_nearest_levels(current_price)
            
            # Calculate S/R target using incremental data
            sr_target = self._calculate_sr_target_incremental(
                current_price=current_price,
                nearest_levels=nearest_levels,
                prices=prices[:i+1]  # Historical prices for touch count
            )
            sr_targets.append(sr_target)
            
            # Debug first few targets
            if len(sr_targets) <= 3:
                sr_levels = sr_calculator.get_current_levels()
                print(f"         Sample {len(sr_targets)}: price={current_price:.4f}, tick_count={sr_calculator.tick_count}")
                print(f"                    Incremental S/R levels: {len(sr_levels['support'])} support, {len(sr_levels['resistance'])} resistance")
                print(f"                    S/R target=[dist_support={sr_target[0]:.4f}, dist_resistance={sr_target[1]:.4f}, support_str={sr_target[2]:.3f}, resistance_str={sr_target[3]:.3f}]")
            
            # Pattern target: probabilistic instead of deterministic
            support_proximity = max(0, 1 - sr_target[0] * 10)  # Closer to support = higher probability
            resistance_proximity = max(0, 1 - sr_target[1] * 10)  # Closer to resistance = higher probability
            
            # Create probabilistic pattern target instead of deterministic
            if support_proximity > 0.7:
                pattern_prob = support_proximity * sr_target[2]  # Weighted by support strength
            elif resistance_proximity > 0.7:
                pattern_prob = -resistance_proximity * sr_target[3]  # Weighted by resistance strength
            else:
                pattern_prob = 0.0  # Neutral zone
            
            pattern_targets.append([pattern_prob])
            
            # Bias target: more complex calculation
            sr_context = (sr_target[2] - sr_target[3]) * (support_proximity - resistance_proximity)
            bias_targets.append([sr_context])
        
        # Convert to numpy arrays
        features_3d = np.array(samples)  # Shape: [n_samples, sequence_length, features_per_timestep]
        
        # VALIDATION: Check for extreme values and NaN/Inf
        if np.isnan(features_3d).any():
            raise ValueError("Features contain NaN values after normalization")
        if np.isinf(features_3d).any():
            raise ValueError("Features contain infinite values after normalization")
        
        # Additional clipping for extreme values
        features_3d = np.clip(features_3d, -10.0, 10.0)  # More conservative clipping
        
        # Prepare for AdaptiveTrainer
        features = features_3d.reshape(features_3d.shape[0], -1)
        
        # Convert targets to arrays
        sr_targets = np.array(sr_targets)
        pattern_targets = np.array(pattern_targets)
        bias_targets = np.array(bias_targets)
        
        # Enhanced target statistics with clear formatting
        print(f"      üìä S/R TARGET ANALYSIS SUMMARY:")
        print(f"      ‚îú‚îÄ Total Samples: {sr_targets.shape[0]:,} √ó {sr_targets.shape[1]} features")
        print(f"      ‚îú‚îÄ Overall Statistics:")
        print(f"      ‚îÇ  ‚îú‚îÄ Mean: {np.mean(sr_targets):.4f}")
        print(f"      ‚îÇ  ‚îú‚îÄ Std Dev: {np.std(sr_targets):.4f}")
        print(f"      ‚îÇ  ‚îî‚îÄ Range: {np.min(sr_targets):.4f} ‚Üí {np.max(sr_targets):.4f}")
        print(f"      ‚îÇ")
        
        # Analyze each target component separately
        dist_support = sr_targets[:, 0]
        dist_resistance = sr_targets[:, 1] 
        support_strength = sr_targets[:, 2]
        resistance_strength = sr_targets[:, 3]
        
        print(f"      ‚îú‚îÄ Target Component Analysis:")
        print(f"      ‚îÇ  ‚îú‚îÄ Support Distance    ‚Üí Mean: {np.mean(dist_support):.4f}, Std: {np.std(dist_support):.4f}")
        print(f"      ‚îÇ  ‚îú‚îÄ Resistance Distance ‚Üí Mean: {np.mean(dist_resistance):.4f}, Std: {np.std(dist_resistance):.4f}")
        print(f"      ‚îÇ  ‚îú‚îÄ Support Strength    ‚Üí Mean: {np.mean(support_strength):.4f}, Std: {np.std(support_strength):.4f}")
        print(f"      ‚îÇ  ‚îî‚îÄ Resistance Strength ‚Üí Mean: {np.mean(resistance_strength):.4f}, Std: {np.std(resistance_strength):.4f}")
        print(f"      ‚îÇ")
        
        # Data quality indicators  
        total_values = sr_targets.size
        zero_values = np.sum(sr_targets == 0)
        fallback_percentage = (zero_values / total_values) * 100
        
        print(f"      ‚îú‚îÄ Data Quality Metrics:")
        print(f"      ‚îÇ  ‚îú‚îÄ Fallback Values: {zero_values:,} / {total_values:,} ({fallback_percentage:.3f}%)")
        print(f"      ‚îÇ  ‚îú‚îÄ Real Data: {total_values - zero_values:,} values ({100-fallback_percentage:.3f}%)")
        print(f"      ‚îÇ  ‚îî‚îÄ Quality Score: {'üü¢ EXCELLENT' if fallback_percentage < 1 else 'üü° GOOD' if fallback_percentage < 5 else 'üî¥ NEEDS REVIEW'}")
        print(f"      ‚îÇ")
        
        # Sample examples with clear interpretation
        if len(sr_targets) > 0:
            print(f"      ‚îî‚îÄ Sample Target Examples:")
            for i in range(min(3, len(sr_targets))):
                sample = sr_targets[i]
                support_dist_pct = sample[0] * 100
                resist_dist_pct = sample[1] * 100
                support_str_pct = sample[2] * 100
                resist_str_pct = sample[3] * 100
                
                # Interpretation
                if support_dist_pct < 1.0:
                    support_status = "üî¥ VERY CLOSE"
                elif support_dist_pct < 3.0:
                    support_status = "üü° NEAR"
                else:
                    support_status = "üü¢ DISTANT"
                    
                if resist_dist_pct < 1.0:
                    resist_status = "üî¥ VERY CLOSE"
                elif resist_dist_pct < 3.0:
                    resist_status = "üü° NEAR"
                else:
                    resist_status = "üü¢ DISTANT"
                
                print(f"         Sample {i+1}:")
                print(f"         ‚îú‚îÄ Support: {support_dist_pct:.1f}% away, {support_str_pct:.1f}% strength {support_status}")
                print(f"         ‚îî‚îÄ Resistance: {resist_dist_pct:.1f}% away, {resist_str_pct:.1f}% strength {resist_status}")
                if i < min(2, len(sr_targets) - 1):
                    print(f"         ‚îÇ")
        
        # Split train/validation (80/20)
        split_idx = int(0.8 * len(features))
        
        return {
            'train_features': features[:split_idx],
            'val_features': features[split_idx:],
            'sr_targets': sr_targets[:split_idx],
            'sr_val_targets': sr_targets[split_idx:],
            'pattern_targets': pattern_targets[:split_idx],
            'pattern_val_targets': pattern_targets[split_idx:],
            'bias_targets': bias_targets[:split_idx],
            'bias_val_targets': bias_targets[split_idx:],
            'feature_count': features.shape[1],  # Total flattened features (sequence_length * features_per_timestep)
            'features_per_timestep': features_per_timestep,  # Features per LSTM timestep
            'sequence_length': sequence_length,
            'total_samples': len(features)
        }
    
    # REMOVED: Old _calculate_support_resistance_levels and _calculate_sr_target methods
    # These used future data and have been replaced by IncrementalSRCalculator
    
    def _calculate_sr_target_incremental(self, current_price: float, nearest_levels: Dict[str, Any], 
                                       prices) -> List[float]:
        """Calculate S/R target using incremental calculator output - PRODUCTION READY"""
        
        # Extract data from incremental calculator
        nearest_support = nearest_levels['support']
        support_strength = nearest_levels['support_strength']
        nearest_resistance = nearest_levels['resistance']
        resistance_strength = nearest_levels['resistance_strength']
        
        # 1. Distance to nearest support
        if nearest_support is not None:
            dist_support = (current_price - nearest_support) / current_price
        else:
            # Dynamic fallback based on recent volatility
            recent_prices = prices[-50:] if len(prices) > 50 else prices
            price_volatility = np.std(recent_prices) / np.mean(recent_prices) if len(recent_prices) > 1 else 0.02
            dist_support = max(0.02, min(0.15, price_volatility * 2))
        
        # 2. Distance to nearest resistance
        if nearest_resistance is not None:
            dist_resistance = (nearest_resistance - current_price) / current_price
        else:
            # Dynamic fallback based on recent volatility
            recent_prices = prices[-50:] if len(prices) > 50 else prices
            price_volatility = np.std(recent_prices) / np.mean(recent_prices) if len(recent_prices) > 1 else 0.02
            dist_resistance = max(0.02, min(0.15, price_volatility * 2))
        
        # 3. Support strength - already calculated by incremental calculator
        if support_strength == 0:  # No support found
            support_strength = 0.3 + np.random.normal(0, 0.05)
            support_strength = max(0.1, min(0.9, support_strength))
        
        # 4. Resistance strength - already calculated by incremental calculator  
        if resistance_strength == 0:  # No resistance found
            resistance_strength = 0.3 + np.random.normal(0, 0.05)
            resistance_strength = max(0.1, min(0.9, resistance_strength))
        
        return [dist_support, dist_resistance, support_strength, resistance_strength]
    
    def _prepare_prediction_data(self, asset_ticks: List[Dict[str, Any]], champion_algorithm: str, model_type: ModelType) -> Dict[str, Any]:
        """Prepare prediction data for ML algorithms from tick data"""
        if not asset_ticks:
            raise ValueError("No asset ticks provided for prediction data preparation")
        
        # Extract recent price and volume data for prediction
        recent_prices = []
        recent_volumes = []
        recent_timestamps = []
        
        # Use last 100 ticks for prediction context
        prediction_window = min(100, len(asset_ticks))
        recent_ticks = asset_ticks[-prediction_window:]
        
        for tick in recent_ticks:
            if 'price' in tick and 'volume' in tick and 'timestamp' in tick:
                recent_prices.append(float(tick['price']))
                recent_volumes.append(float(tick['volume']))
                recent_timestamps.append(tick['timestamp'])
        
        if not recent_prices:
            raise ValueError("No valid recent price data found for predictions")
        
        # Create prediction dataset in format expected by algorithms
        prediction_data = {
            'price_history': recent_prices,
            'volume_history': recent_volumes,
            'timestamps': recent_timestamps,
            'current_price': recent_prices[-1] if recent_prices else 0.0,
            'current_volume': recent_volumes[-1] if recent_volumes else 0.0,
            'asset': 'PREDICTION_ASSET',  # Will be overridden by caller
            'metadata': {
                'champion_algorithm': champion_algorithm,
                'model_type': model_type.value,
                'prediction_window': prediction_window,
                'prediction_time': datetime.now().isoformat()
            }
        }
        
        return prediction_data


# Factory function
def create_advanced_market_analyzer(data_path: str = "./test_analyzer_data", 
                                  config_manager=None) -> AdvancedMarketAnalyzer:
    """Factory function per creare AdvancedMarketAnalyzer"""
    return AdvancedMarketAnalyzer(data_path, config_manager)


# Export
__all__ = [
    'AdvancedMarketAnalyzer',
    'create_advanced_market_analyzer'
]